{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b92dc2",
   "metadata": {},
   "source": [
    "# Code for CS598 Final Project \n",
    "Author: Binbin Weng (binbinw2@illinois.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399241b",
   "metadata": {},
   "source": [
    "## The original Paper:\n",
    "The method I am implementing in this project is from the paper, `A disease inference method based on symptom extraction and bidirectional Long Short Term Memory networks`, which introduces a method of multi-label classifier for disease inference with clinical text data. I didn't find the repo of the original paper. With the help of some websites including https://pytorch.org/, https://numpy.org/, https://pandas.pydata.org/, https://stackoverflow.com/, https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/documentation/Installation.html etc, I write the following code. Please follow the instructions in the Markdown sections to get the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa01d93",
   "metadata": {},
   "source": [
    "### Step  1: Prepare the data\n",
    "In this project, two tables will be used, `NOTEEVENTS` and `DIAGNOSES_ICD` from MIMIC-III. Please follow the instruction in the page https://physionet.org/content/mimiciii/1.4/ to download the two tables.\n",
    "After downloading these two tables, please save the tables to the same folder as you save the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30659925",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the data for Batch MetaMap:\n",
    "The purpose of the original paper is to use the clinical text data to build a multi-label classifier for disease inference. The first thing needs to do is to extract symptoms from the clinical text data. It uses the techniques introduced by MetaMap to extract symptoms from the clinical text data. \n",
    "\n",
    "MetaMap provides a database which you can download and set up the environment for it so that you can run the extraction from your local side. I tried this method, which was very slow. It took about 25 CPU hours to extract symptoms from about 1000 records. The data contains about 50,000 records, so obviously this method is not efficient enough. But you still can try this method. Please go to the website page, https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/run-locally/MainDownload.html, to download related database and follow the instructions to set up the environment.  \n",
    "\n",
    "Luckily, MetaMap also provides the service of Batch MetaMap, where you can upload your file and it will help extract symptoms from your file and return results to you. So the following part of code is to prepare the data which you can upload to Batch MetaMap. After running the following part of code, you will see some statistics of the original datasets, and two files will be generated, `merged_data.csv` and `data_for_metamapBatch_full.txt`. The file, `merged_data.csv` is for later modeling use which contains labels for each records. The file, `data_for_metamapBatch_full.txt`, is to submit in Batch MetaMap to get the symptoms for each records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d0eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wangguanshen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6febb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/85wt69jn4hlfh9nykgpdzzg80000gn/T/ipykernel_44909/2333017314.py:2: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  noteevents = pd.read_csv('NOTEEVENTS.csv.gz')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of NOTEEVENTS table is:  (2083180, 11)\n",
      "Categories of the clinical text:\n",
      "Nursing/other        822497\n",
      "Radiology            522279\n",
      "Nursing              223556\n",
      "ECG                  209051\n",
      "Physician            141624\n",
      "Discharge summary     59652\n",
      "Echo                  45794\n",
      "Respiratory           31739\n",
      "Nutrition              9418\n",
      "General                8301\n",
      "Rehab Services         5431\n",
      "Social Work            2670\n",
      "Case Management         967\n",
      "Pharmacy                103\n",
      "Consult                  98\n",
      "Name: CATEGORY, dtype: int64\n",
      "The shape of Discharge Summary table is:  (59652, 11)\n",
      "Number of unique patients in the discharge summary table:  41127\n",
      "Number of unique visits in the discharge summary table:  52726\n"
     ]
    }
   ],
   "source": [
    "########## processing NOTEVENTS Table #######################################\n",
    "noteevents = pd.read_csv('NOTEEVENTS.csv.gz')\n",
    "print(\"The shape of NOTEEVENTS table is: \", noteevents.shape)\n",
    "print(\"Categories of the clinical text:\")\n",
    "print(noteevents['CATEGORY'].value_counts())\n",
    "# select only data with the category of Discharge summary\n",
    "discharge_summary_data = noteevents[noteevents['CATEGORY']=='Discharge summary']\n",
    "print(\"The shape of Discharge Summary table is: \", discharge_summary_data.shape)\n",
    "# subject_id is patient id, hadm_id is visit id\n",
    "print(\"Number of unique patients in the discharge summary table: \",discharge_summary_data['SUBJECT_ID'].nunique())\n",
    "print(\"Number of unique visits in the discharge summary table: \",discharge_summary_data['HADM_ID'].nunique())\n",
    "#drop unnecessary columns from discharge summary data\n",
    "discharge_summary_text = discharge_summary_data.drop(['ROW_ID','CHARTDATE','CHARTTIME','STORETIME','CATEGORY','DESCRIPTION','CGID','ISERROR'],axis =1)\n",
    "#change datatype for following processing\n",
    "discharge_summary_text['SUBJECT_ID'] = discharge_summary_text['SUBJECT_ID'].astype(str)\n",
    "# function to concatenate contents in each column with the same 'HADM_ID'\n",
    "def concat_values(group):\n",
    "    return pd.Series({\n",
    "        'SUBJECT_concat': ' '.join(group['SUBJECT_ID']),\n",
    "        'TEXT_concat': ' '.join(group['TEXT'])\n",
    "    })\n",
    "discharge_summary_text = discharge_summary_text.groupby('HADM_ID').apply(concat_values)\n",
    "discharge_summary_text = discharge_summary_text.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac3cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of DIAGNOSES_ICD table is:  (651047, 5)\n",
      "Number of unique patients in the DIAGNOSES_ICD table:  46520\n",
      "Number of unique visits in the DIAGNOSES_ICD table:  58976\n"
     ]
    }
   ],
   "source": [
    "########## processing DIAGNOSES_ICD Table #######################################\n",
    "dig = pd.read_csv('DIAGNOSES_ICD.csv.gz')\n",
    "print(\"The shape of DIAGNOSES_ICD table is: \",dig.shape)\n",
    "print(\"Number of unique patients in the DIAGNOSES_ICD table: \", dig['SUBJECT_ID'].nunique())\n",
    "print(\"Number of unique visits in the DIAGNOSES_ICD table: \", dig['HADM_ID'].nunique())\n",
    "# drop unnecessary columns from diagnoses data\n",
    "diagnoses_data = dig.drop(['ROW_ID','SEQ_NUM'],axis = 1)\n",
    "#change datatype for following processing\n",
    "diagnoses_data['SUBJECT_ID'] = diagnoses_data['SUBJECT_ID'].astype(str)\n",
    "diagnoses_data['ICD9_CODE'] = diagnoses_data['ICD9_CODE'].astype(str)\n",
    "# function to concatenate contents in each column with the same 'HADM_ID'\n",
    "def concat_values2(group):\n",
    "    return pd.Series({\n",
    "        'SUBJECT_concat': ' '.join(group['SUBJECT_ID']),\n",
    "        'ICD_concat_CODEs': ','.join(group['ICD9_CODE'])\n",
    "    })\n",
    "diagnoses_data = diagnoses_data.groupby('HADM_ID').apply(concat_values2)\n",
    "diagnoses_data = diagnoses_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0da0b591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the merged data table is:  (52726, 6)\n"
     ]
    }
   ],
   "source": [
    "########## merging the two tables and processing the merged table #######################################\n",
    "# inner join the two dataframe on HADM_ID\n",
    "merged_data = pd.merge(discharge_summary_text,diagnoses_data,on = 'HADM_ID',how = 'inner')\n",
    "# basic processing the discharge summaries\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def text_processing(text):\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\[[^\\]]*\\]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text.strip())\n",
    "    negations = ['no', 'not', 'never', 'none', 'nobody', 'nothing', \n",
    "                 'neither', 'nor', 'nowhere', 'cannot', 'can\\'t', \n",
    "                 'doesn\\'t', 'don\\'t', 'won\\'t', 'shouldn\\'t', \n",
    "                 'couldn\\'t', 'wouldn\\'t', 'isn\\'t', 'aren\\'t', 'wasn\\'t', \n",
    "                 'weren\\'t', 'didn\\'t', 'hadn\\'t', 'hasn\\'t', 'haven\\'t',\n",
    "                 'denies', 'without','denied']\n",
    "    adv_conjs = ['but','yet','however','nevertheless','still','on the other hand',\n",
    "                 'in contrast','in contrast','notwithstanding','although','despite that',\n",
    "                 'even though', 'in spite of','nonetheless', 'despite']\n",
    "    negation_pattern = \"(?:\" + \"|\".join(negations) + \")\"\n",
    "    adv_conj_pattern = \"(?:\" + \"|\".join(adv_conjs) + \")\"\n",
    "    regex = r'\\b(' + '|'.join(negations) + r')\\b\\s*(.*?)\\s*(?=\\b(' + '|'.join(adv_conjs) + r')\\b|\\.|$)'\n",
    "    text = re.sub(regex, \"\", text, flags=re.IGNORECASE)\n",
    "    text = \" \".join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    text = re.sub('\\s+', ' ', text.strip())\n",
    "    return text\n",
    "merged_data['processed_text']=merged_data['TEXT_concat'].apply(text_processing)\n",
    "merged_data.to_csv('merged_data.csv',index = False)\n",
    "print(\"The shape of the merged data table is: \", merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e5a984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## generating the discharge summaries for Batch MetaMap #######################################\n",
    "data_for_metamapBatch = merged_data.loc[:,['processed_text']]\n",
    "for i in range(merged_data.shape[0]):\n",
    "    data_for_metamapBatch['processed_text'][i] = str(i)+'|'+data_for_metamapBatch['processed_text'][i]\n",
    "data_for_metamapBatch.to_csv('data_for_metamapBatch_full.txt',sep = '\\n',index = False,header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08b514",
   "metadata": {},
   "source": [
    "### Step 3: Upload the file above to Batch MetaMap and get symptoms\n",
    "Register an account on National Library of Medicine, https://www.nlm.nih.gov/. It may take some time to review your registration.\n",
    "\n",
    "After your registration gets approved, go to https://ii.nlm.nih.gov/Batch/UTS_Required/MetaMap.html,Enter, enter FULL Email Address (which is used to contact you when the job is finished), upload the data_for_metamapBatch_full.txt from the previous step, in the Out/Display Options select Fielded MMI output (-N), in the Batch Specific Options select Single Line Delimited Input w/ ID, in the I would like to only use specific Semantic Types, enter `sosy,dsyn,neop,fngs,bact,virs,cgab,acab,lbtr,inpo,mobd,comd,anab`, then submit Batch MetaMap. It then will give you a link to track your job. It takes about 3 days to process the data. When the job finishes, you will receive an email containing link for the output files. Download the `text.out` file and `text.out.ERR` file to the same folder you store your other data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccedf9a",
   "metadata": {},
   "source": [
    "### Step 4: Prepare data for modeling\n",
    "After we get the results from Batch MetaMap, we need to process the results and merge the results with the `merged_data` that we generated in step 2 to prepare data for the modeling. The follwing part of code is to prepare the data, including merging the symptoms with the lables, calculating TF-IDF scores for each symptoms, building word2vec model, generating forward and backward X because we will build BiLSTMa in the later part. After running this part of code, several files will be generated,`symptom_ICD.csv`, `tfidf_matrix.csv`, `word2vec_model`. These files are saved just in case the program is stopped unexpectedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f960ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7eb523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/85wt69jn4hlfh9nykgpdzzg80000gn/T/ipykernel_44909/2954109733.py:2: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  symptoms = pd.read_csv('text.out.txt', delimiter='|', header=None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of symptoms table is:  (47887, 3)\n"
     ]
    }
   ],
   "source": [
    "################## read and process the results data from Batch MetaMap #########################\n",
    "symptoms = pd.read_csv('text.out.txt', delimiter='|', header=None,\n",
    "                       usecols=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "symptom_copy = symptoms\n",
    "emptyrow=symptoms[8].isnull()\n",
    "for i in range(symptoms.shape[0]):\n",
    "    if emptyrow[i]:\n",
    "        s = symptoms.loc[i,0].split('|')\n",
    "        symptom_copy.loc[i,:len(s)-1]=s\n",
    "num_list = [str(i) for i in range(merged_data.shape[0])]\n",
    "l = []\n",
    "for i in range(symptom_copy.shape[0]):\n",
    "    if symptom_copy.loc[i,0] not in num_list:\n",
    "        if '\\n' in symptom_copy.loc[i,0]:\n",
    "            #l.append(i)\n",
    "            s = symptom_copy.loc[i,0].split('\\n')\n",
    "            if s[1] in num_list:\n",
    "                symptom_copy.loc[i,0] = s[1]\n",
    "            else:\n",
    "                l.append(i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "symptom_copy2 = symptom_copy.drop(index = l)\n",
    "def concat_values(group):\n",
    "    return pd.Series({\n",
    "        'symptom_code': ','.join(group[4]),\n",
    "        'symptom': ','.join(group[3])\n",
    "    })\n",
    "symptoms_final = symptom_copy2.groupby(0).apply(concat_values)\n",
    "symptoms_final = symptoms_final.reset_index()\n",
    "symptoms_final = symptoms_final.rename(columns = {0:'index'})\n",
    "symptoms_final['index']=symptoms_final['index'].astype(int)\n",
    "symptoms_final = symptoms_final.sort_values(by='index')\n",
    "symptoms_final = symptoms_final.reset_index()\n",
    "symptoms_final = symptoms_final.drop('level_0',axis = 1)\n",
    "print(\"The shape of symptoms table is: \", symptoms_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee8b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique visits after merging with symptoms is:  (47887, 9)\n"
     ]
    }
   ],
   "source": [
    "################## merged the symptoms with the ICD codes with each visit ##########################\n",
    "merged_data = pd.read_csv('merged_data.csv')\n",
    "merged_data = merged_data.reset_index()\n",
    "symptom_ICD = pd.merge(merged_data,symptoms_final,on = 'index',how = 'inner')\n",
    "symptom_ICD.to_csv('symptom_ICD.csv',index = False)\n",
    "print(\"The number of unique visits after merging with symptoms is: \", symptom_ICD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "537d344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## set seeds ##########################\n",
    "seed = 2023\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30799131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of symptoms of a visit:  228\n",
      "minimum number of symptoms of a visit:  1\n"
     ]
    }
   ],
   "source": [
    "# select the first 50 symptoms if the number of symptoms a visit has is over 50\n",
    "# and delete the records with only 1 symptom per the paper\n",
    "symptom_ICD['symptom_code_list'] = symptom_ICD['symptom_code'].apply(lambda x:x.split(','))\n",
    "symptom_ICD['symptom_code_list'] = symptom_ICD['symptom_code_list'].apply(lambda x:list(set(x)))\n",
    "code_num = symptom_ICD['symptom_code_list'].apply(lambda x:len(x))\n",
    "print(\"maximum number of symptoms of a visit: \",code_num.max())\n",
    "print(\"minimum number of symptoms of a visit: \",code_num.min())\n",
    "symptom_ICD['selected_symtom_code'] = [lst[:50] for lst in symptom_ICD['symptom_code_list']]\n",
    "selected_code_num = symptom_ICD['selected_symtom_code'].apply(lambda x:len(x))\n",
    "symptom_ICD=symptom_ICD[symptom_ICD['selected_symtom_code'].apply(lambda x:len(x)>=2)]\n",
    "symptom_ICD = symptom_ICD.reset_index(drop = True)\n",
    "\n",
    "# keep the first 3 digits of the ICD code and\n",
    "# select the most common 50 ICDs and drop the rest (in the original paper, the most common 100 ICDs is also used)\n",
    "symptom_ICD['icds'] = symptom_ICD['ICD_concat_CODEs'].astype(str).apply(lambda x: x.split(',')).apply(lambda y: [item[:3] for item in y])\n",
    "symptom_ICD['icds'] =symptom_ICD['icds'].apply(lambda x:list(set(x))) \n",
    "df_exploded = symptom_ICD.explode('icds')\n",
    "common50_disease = list(df_exploded['icds'].value_counts()[:50].index)\n",
    "symptom_ICD['common50_icds'] = symptom_ICD['icds'].apply(lambda x: [item for item in x if item in common50_disease])\n",
    "symptom_ICD = symptom_ICD[symptom_ICD['common50_icds'].apply(lambda x:len(x)>0)]\n",
    "symptom_ICD = symptom_ICD.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d546338",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### generate dummy variables for symptoms and ICD codes\n",
    "s1 = symptom_ICD.shape[1]\n",
    "symptom_ICD_new = (symptom_ICD['symptom_code_list'].apply(lambda x:','.join(x))).str.get_dummies(',')\n",
    "symptom_ICD_new = symptom_ICD.join(symptom_ICD_new)\n",
    "e1 = symptom_ICD_new.shape[1]\n",
    "\n",
    "s2 = symptom_ICD_new.shape[1]\n",
    "symptom_ICD_new = symptom_ICD_new.join((symptom_ICD_new['common50_icds'].apply(lambda x:','.join(x))).str.get_dummies(','))\n",
    "e2 = symptom_ICD_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a45692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (46659, 18146)\n",
      "Y shape:  (46659, 50)\n",
      "shape of W matrix:  (18146, 50)\n",
      "time spent on calculating TF-IDF matrix:  5243.654246807098\n"
     ]
    }
   ],
   "source": [
    "################## calculate TF-IDF Matrix #########################################\n",
    "start_time = time.time()\n",
    "X = np.array(symptom_ICD_new.iloc[:,s1:e1])\n",
    "Y = np.array(symptom_ICD_new.iloc[:,s2:e2])\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", Y.shape)\n",
    "item_tf =  np.zeros((X.shape[1],Y.shape[1]))\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(Y.shape[1]):\n",
    "        item_tf[i,j] = sum(X[:,i]*Y[:,j])\n",
    "D = []\n",
    "for i in range(X.shape[1]):\n",
    "    ans = 0\n",
    "    for j in range(Y.shape[1]):\n",
    "        if sum(X[:,i]*Y[:,j])>0:\n",
    "            ans+=1\n",
    "    D.append(ans)\n",
    "W = np.zeros((X.shape[1],Y.shape[1]))\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(Y.shape[1]):\n",
    "        W[i,j]=item_tf[i,j]*math.log(Y.shape[1]/D[i])\n",
    "print(\"shape of W matrix: \",W.shape)\n",
    "np.savetxt(\"tfidf_matrix.csv\",W,delimiter = ',')\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on calculating TF-IDF matrix: \", diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b406573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on word2vec:  18.809247970581055\n"
     ]
    }
   ],
   "source": [
    "################## build word2vec model #########################################\n",
    "start_time = time.time()\n",
    "sentences = symptom_ICD_new['symptom_code_list'].apply(lambda x: [y.replace(\" \",\"\") for y in x])\n",
    "embedding_model = Word2Vec(sentences, sg=1, window=5, vector_size=128, min_count=1, workers=4)\n",
    "embedding_model.save('word2vec_model')\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on word2vec: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db4ca82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## prepare X ###################################################\n",
    "numOfICD = 50\n",
    "# the representation of TF-IDF\n",
    "col = list(symptom_ICD_new.columns)\n",
    "col = col[s1:e1]\n",
    "col_map = {}\n",
    "for i in range(len(col)):\n",
    "    col_map[col[i].replace(\" \",\"\")] = i\n",
    "symptom_ICD_new['tfidf_x'] = list(np.zeros((symptom_ICD_new.shape[0],50,numOfICD)))\n",
    "symptom_ICD_new['tfidf_x_rev'] = list(np.zeros((symptom_ICD_new.shape[0],50,numOfICD)))\n",
    "for i in range(symptom_ICD_new.shape[0]): \n",
    "    l = [x.replace(\" \",\"\") for x in symptom_ICD_new['selected_symtom_code'][i]]\n",
    "    for j in range(len(l)):\n",
    "        symptom_ICD_new['tfidf_x'][i][j] = W[col_map[l[j]],:]\n",
    "        symptom_ICD_new['tfidf_x_rev'][i][len(l)-j-1]= W[col_map[l[j]],:]\n",
    "        \n",
    "# the representation of word2vec\n",
    "symptom_ICD_new['word2vec_x'] = list(np.zeros((symptom_ICD_new.shape[0],50,128)))\n",
    "symptom_ICD_new['word2vec_x_rev'] = list(np.zeros((symptom_ICD_new.shape[0],50,128)))\n",
    "for i in range(symptom_ICD_new.shape[0]): \n",
    "    l = [x.replace(\" \",\"\") for x in symptom_ICD_new['selected_symtom_code'][i]]\n",
    "    for j in range(len(l)):\n",
    "        symptom_ICD_new['word2vec_x'][i][j] = embedding_model.wv[l[j]]\n",
    "        symptom_ICD_new['word2vec_x_rev'][i][len(l)-j-1]= embedding_model.wv[l[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1390ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## prepare Y ###################################################\n",
    "symptom_ICD_new['y'] = symptom_ICD_new.apply(lambda row: row.iloc[s2:e2].to_list(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9067a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### dataframe for X and Y #######################################\n",
    "final_df = symptom_ICD_new.loc[:,['tfidf_x','tfidf_x_rev','word2vec_x','word2vec_x_rev','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75d109e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/85wt69jn4hlfh9nykgpdzzg80000gn/T/ipykernel_44909/4012697825.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1678402353079/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  X_forward_tfidf = torch.tensor(final_df[\"tfidf_x\"]).float()\n"
     ]
    }
   ],
   "source": [
    "######################## split data into training (80%) and testing data (20%) #################\n",
    "X_forward_tfidf = torch.tensor(final_df[\"tfidf_x\"]).float()\n",
    "X_backward_tfidf = torch.tensor(final_df[\"tfidf_x_rev\"]).float()\n",
    "X_forward_word2vec = torch.tensor(final_df[\"word2vec_x\"]).float()\n",
    "X_backward_word2vec = torch.tensor(final_df[\"word2vec_x_rev\"]).float()\n",
    "Y = torch.tensor(final_df[\"y\"].tolist()).float()\n",
    "X_train_forward_tfidf, X_test_forward_tfidf, X_train_backward_tfidf, X_test_backward_tfidf,X_train_forward_word2vec, X_test_forward_word2vec, X_train_backward_word2vec, X_test_backward_word2vec, y_train, y_test = train_test_split(X_forward_tfidf, X_backward_tfidf, X_forward_word2vec, X_backward_word2vec, Y, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024be30e",
   "metadata": {},
   "source": [
    "### Step 5: Modeling, evaluating and comparing\n",
    "After prepare the data for modeling, in the following part, we will build and compare several models with the prepared data. \n",
    "\n",
    "The models are:\n",
    "1. the bidirectional LSTM with the X representation of TF-IDF\n",
    "2. the bidirectional LSTM with the X representation of word2vec\n",
    "3. simply combine the results from the two models above\n",
    "4. training two bidirectional LSTMs with the X representations of both TF-IDF and word2vec together\n",
    "5. training two bidirectional GRUs with the X representations of both TF-IDF and word2vec together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7d83c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, forward_input, backward_input):\n",
    "        h0_forward = torch.zeros(self.num_layers, forward_input.size(0), self.hidden_size)\n",
    "        c0_forward = torch.zeros(self.num_layers, forward_input.size(0), self.hidden_size)\n",
    "        h0_backward = torch.zeros(self.num_layers, backward_input.size(0), self.hidden_size)\n",
    "        c0_backward = torch.zeros(self.num_layers, backward_input.size(0), self.hidden_size)\n",
    "        out_forward, _ = self.lstm(forward_input, (h0_forward, c0_forward))\n",
    "        out_backward, _ = self.lstm(backward_input, (h0_backward, c0_backward))\n",
    "        output = torch.cat((out_forward[:,-1,:], out_backward[:,0,:]), dim=1)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba674d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(pred,truth,cutoff):\n",
    "    label = np.array(((pred>cutoff)*1.0))\n",
    "    truth2 = np.array(truth)\n",
    "    acc = np.sum(label==truth2)/np.prod(truth2.shape)\n",
    "    mip = np.sum(label*truth2)/np.sum(label)\n",
    "    mir = np.sum(label*truth2)/np.sum(truth2)\n",
    "    mif1 = (2*mip*mir)/(mip+mir)\n",
    "    auc = roc_auc_score(truth2,np.array(pred))\n",
    "    return [acc,mip,mir,mif1,auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6804b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Model', 'Accuracy', 'MiP', 'MiR', 'MiF1', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df6371e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/100] loss: 0.321\n",
      "[Epoch 20/100] loss: 0.315\n",
      "[Epoch 30/100] loss: 0.309\n",
      "[Epoch 40/100] loss: 0.303\n",
      "[Epoch 50/100] loss: 0.300\n",
      "[Epoch 60/100] loss: 0.297\n",
      "[Epoch 70/100] loss: 0.294\n",
      "[Epoch 80/100] loss: 0.292\n",
      "[Epoch 90/100] loss: 0.291\n",
      "[Epoch 100/100] loss: 0.289\n",
      "time spent on training the model1 is:  2451.0875160694122\n"
     ]
    }
   ],
   "source": [
    "##################### build model1: the bidirectional LSTM with the X representation of TF-IDF ################\n",
    "start_time = time.time()\n",
    "train_data = torch.utils.data.TensorDataset(X_train_forward_tfidf, X_train_backward_tfidf, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "model = BiLSTM(X_train_forward_tfidf.shape[2], 100, 1,y_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs_forward, inputs_backward, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs_forward, inputs_backward)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print('[Epoch %d/100] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/i))\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on training the model1 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5a74188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model1 is:  1.9696598052978516\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_tfidf = model(X_test_forward_tfidf, X_test_backward_tfidf)\n",
    "y_pred_tfidf = y_pred_tfidf.detach().numpy()\n",
    "rslt = evaluateModel(y_pred_tfidf,y_test,0.2)\n",
    "results.loc[len(results.index)]=['BiLSTM with TF-IDF',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model1 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fc6a161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  Accuracy       MiP       MiR      MiF1       AUC\n",
      "0  BiLSTM with TF-IDF   0.84488  0.418782  0.595707  0.491817  0.804495\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d98c5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/{num_epochs}] loss: 0.316\n",
      "[Epoch 20/{num_epochs}] loss: 0.308\n",
      "[Epoch 30/{num_epochs}] loss: 0.301\n",
      "[Epoch 40/{num_epochs}] loss: 0.297\n",
      "[Epoch 50/{num_epochs}] loss: 0.293\n",
      "[Epoch 60/{num_epochs}] loss: 0.290\n",
      "[Epoch 70/{num_epochs}] loss: 0.288\n",
      "[Epoch 80/{num_epochs}] loss: 0.285\n",
      "[Epoch 90/{num_epochs}] loss: 0.283\n",
      "[Epoch 100/{num_epochs}] loss: 0.281\n",
      "time spent on training model2 is:  3197.1380009651184\n"
     ]
    }
   ],
   "source": [
    "##################### build model2: the bidirectional LSTM with the X representation of word2vec ################\n",
    "start_time = time.time()\n",
    "train_data = torch.utils.data.TensorDataset(X_train_forward_word2vec, X_train_backward_word2vec, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "model2 = BiLSTM(X_train_forward_word2vec.shape[2], 100, 1,y_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(),lr = 0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs_forward, inputs_backward, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(inputs_forward, inputs_backward)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print('[Epoch %d/{num_epochs}] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/i))\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on training model2 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bda2d319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model is:  2.74979305267334\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_word2vec = model2(X_test_forward_word2vec, X_test_backward_word2vec)\n",
    "y_pred_word2vec = y_pred_word2vec.detach().numpy()\n",
    "rslt = evaluateModel(y_pred_word2vec,y_test,0.2)\n",
    "results.loc[len(results.index)]=['BiLSTM with word2vec',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ea6a645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model  Accuracy       MiP       MiR      MiF1       AUC\n",
      "0    BiLSTM with TF-IDF  0.844880  0.418782  0.595707  0.491817  0.804495\n",
      "1  BiLSTM with word2vec  0.861234  0.460096  0.583818  0.514625  0.814443\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b2b037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model3 is:  0.13937115669250488\n"
     ]
    }
   ],
   "source": [
    "################## build model3: simply combine the results from model1 and model2 ################################\n",
    "start_time = time.time()\n",
    "rslt = evaluateModel(y_pred_tfidf*0.5+y_pred_word2vec*0.5,y_test,0.2)\n",
    "results.loc[len(results.index)]=['BiLSTM with TF-IDF and word2vec',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model3 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eea7bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model  Accuracy       MiP       MiR      MiF1  \\\n",
      "0               BiLSTM with TF-IDF  0.844880  0.418782  0.595707  0.491817   \n",
      "1             BiLSTM with word2vec  0.861234  0.460096  0.583818  0.514625   \n",
      "2  BiLSTM with TF-IDF and word2vec  0.859053  0.455739  0.610504  0.521890   \n",
      "\n",
      "        AUC  \n",
      "0  0.804495  \n",
      "1  0.814443  \n",
      "2  0.829092  \n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79cd6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build model4: training two bidirectional LSTMs with the X representations of both TF-IDF and word2vec together ################################\n",
    "class CombinedBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, input_size2,hidden_size, num_layers,output_size):\n",
    "        super(CombinedBiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_tfidf = nn.LSTM(input_size, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.lstm_word2vec = nn.LSTM(input_size2, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, forward_input_tfidf, backward_input_tfidf,forward_input_word2vec, backward_input_word2vec):\n",
    "        h0_forward_tfidf = torch.zeros(self.num_layers, forward_input_tfidf.size(0), self.hidden_size)\n",
    "        c0_forward_tfidf = torch.zeros(self.num_layers, forward_input_tfidf.size(0), self.hidden_size)\n",
    "        h0_backward_tfidf = torch.zeros(self.num_layers, backward_input_tfidf.size(0), self.hidden_size)\n",
    "        c0_backward_tfidf = torch.zeros(self.num_layers, backward_input_tfidf.size(0), self.hidden_size)\n",
    "        out_forward_tfidf, _ = self.lstm_tfidf(forward_input_tfidf, (h0_forward_tfidf, c0_forward_tfidf))\n",
    "        out_backward_tfidf, _ = self.lstm_tfidf(backward_input_tfidf, (h0_backward_tfidf, c0_backward_tfidf))\n",
    "        output_tfidf = torch.cat((out_forward_tfidf[:,-1,:], out_backward_tfidf[:,0,:]), dim=1)\n",
    "        output_tfidf = self.dropout(output_tfidf)\n",
    "        output_tfidf = self.fc(output_tfidf)\n",
    "        output_tfidf = self.sigmoid(output_tfidf)\n",
    "        \n",
    "        h0_forward_word2vec = torch.zeros(self.num_layers, forward_input_word2vec.size(0), self.hidden_size)\n",
    "        c0_forward_word2vec = torch.zeros(self.num_layers, forward_input_word2vec.size(0), self.hidden_size)\n",
    "        h0_backward_word2vec = torch.zeros(self.num_layers, backward_input_word2vec.size(0), self.hidden_size)\n",
    "        c0_backward_word2vec = torch.zeros(self.num_layers, backward_input_word2vec.size(0), self.hidden_size)\n",
    "        out_forward_word2vec, _ = self.lstm_word2vec(forward_input_word2vec, (h0_forward_word2vec, c0_forward_word2vec))\n",
    "        out_backward_word2vec, _ = self.lstm_word2vec(backward_input_word2vec, (h0_backward_word2vec, c0_backward_word2vec))\n",
    "        output_word2vec = torch.cat((out_forward_word2vec[:,-1,:], out_backward_word2vec[:,0,:]), dim=1)\n",
    "        output_word2vec = self.dropout(output_word2vec)\n",
    "        output_word2vec = self.fc(output_word2vec)\n",
    "        output_word2vec = self.sigmoid(output_word2vec)\n",
    "        \n",
    "        output = output_word2vec*0.5+output_tfidf*0.5\n",
    "                                    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d8618f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/100] loss: 0.310\n",
      "[Epoch 20/100] loss: 0.299\n",
      "[Epoch 30/100] loss: 0.292\n",
      "[Epoch 40/100] loss: 0.286\n",
      "[Epoch 50/100] loss: 0.281\n",
      "[Epoch 60/100] loss: 0.276\n",
      "[Epoch 70/100] loss: 0.273\n",
      "[Epoch 80/100] loss: 0.271\n",
      "[Epoch 90/100] loss: 0.268\n",
      "[Epoch 100/100] loss: 0.267\n",
      "time spent on training the model4 is:  5808.835932016373\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_data = torch.utils.data.TensorDataset(X_train_forward_tfidf, X_train_backward_tfidf,X_train_forward_word2vec, X_train_backward_word2vec, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "model3 = CombinedBiLSTM(X_train_forward_tfidf.shape[2],X_train_forward_word2vec.shape[2], 100,1, y_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(),lr = 0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs_forward_tfidf, inputs_backward_tfidf,inputs_forward_word2vec, inputs_backward_word2vec, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model3(inputs_forward_tfidf, inputs_backward_tfidf,inputs_forward_word2vec, inputs_backward_word2vec)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print('[Epoch %d/100] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/i))\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on training the model4 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9115c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model4 is:  4.616988182067871\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model3.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_combined = model3(X_test_forward_tfidf, X_test_backward_tfidf,\n",
    "                             X_test_forward_word2vec, X_test_backward_word2vec)\n",
    "y_pred_combined = y_pred_combined.detach().numpy()\n",
    "rslt = evaluateModel(y_pred_combined,y_test,0.2)\n",
    "results.loc[len(results.index)]=['Combined training of BiLSTMs',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model4 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abf8a7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model  Accuracy       MiP       MiR      MiF1  \\\n",
      "0               BiLSTM with TF-IDF  0.844880  0.418782  0.595707  0.491817   \n",
      "1             BiLSTM with word2vec  0.861234  0.460096  0.583818  0.514625   \n",
      "2  BiLSTM with TF-IDF and word2vec  0.859053  0.455739  0.610504  0.521890   \n",
      "3     Combined training of BiLSTMs  0.869381  0.486158  0.643076  0.553714   \n",
      "\n",
      "        AUC  \n",
      "0  0.804495  \n",
      "1  0.814443  \n",
      "2  0.829092  \n",
      "3  0.842082  \n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed658c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build model5: training two bidirectional GRUs with the X representations of both TF-IDF and word2vec together ################################\n",
    "class CombinedBiGRU(nn.Module):\n",
    "    def __init__(self, input_size, input_size2,hidden_size, num_layers,output_size):\n",
    "        super(CombinedBiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru_tfidf = nn.GRU(input_size, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.gru_word2vec = nn.GRU(input_size2, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, forward_input_tfidf, backward_input_tfidf,forward_input_word2vec, backward_input_word2vec):\n",
    "        h0_forward_tfidf = torch.zeros(self.num_layers, forward_input_tfidf.size(0), self.hidden_size)\n",
    "        h0_backward_tfidf = torch.zeros(self.num_layers, backward_input_tfidf.size(0), self.hidden_size)\n",
    "        out_forward_tfidf, _ = self.gru_tfidf(forward_input_tfidf, h0_forward_tfidf)\n",
    "        out_backward_tfidf, _ = self.gru_tfidf(backward_input_tfidf, h0_backward_tfidf)\n",
    "        output_tfidf = torch.cat((out_forward_tfidf[:,-1,:], out_backward_tfidf[:,0,:]), dim=1)\n",
    "        output_tfidf = self.dropout(output_tfidf)\n",
    "        output_tfidf = self.fc(output_tfidf)\n",
    "        output_tfidf = self.sigmoid(output_tfidf)\n",
    "        \n",
    "        h0_forward_word2vec = torch.zeros(self.num_layers, forward_input_word2vec.size(0), self.hidden_size)\n",
    "        h0_backward_word2vec = torch.zeros(self.num_layers, backward_input_word2vec.size(0), self.hidden_size)\n",
    "        out_forward_word2vec, _ = self.gru_word2vec(forward_input_word2vec, h0_forward_word2vec)\n",
    "        out_backward_word2vec, _ = self.gru_word2vec(backward_input_word2vec, h0_backward_word2vec)\n",
    "        output_word2vec = torch.cat((out_forward_word2vec[:,-1,:], out_backward_word2vec[:,0,:]), dim=1)\n",
    "        output_word2vec = self.dropout(output_word2vec)\n",
    "        output_word2vec = self.fc(output_word2vec)\n",
    "        output_word2vec = self.sigmoid(output_word2vec)\n",
    "        \n",
    "        output = output_word2vec*0.5+output_tfidf*0.5                       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "710d1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/100] loss: 0.306\n",
      "[Epoch 20/100] loss: 0.293\n",
      "[Epoch 30/100] loss: 0.283\n",
      "[Epoch 40/100] loss: 0.277\n",
      "[Epoch 50/100] loss: 0.272\n",
      "[Epoch 60/100] loss: 0.269\n",
      "[Epoch 70/100] loss: 0.266\n",
      "[Epoch 80/100] loss: 0.264\n",
      "[Epoch 90/100] loss: 0.263\n",
      "[Epoch 100/100] loss: 0.261\n",
      "time spent on training the model4 is:  5426.055936098099\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_data = torch.utils.data.TensorDataset(X_train_forward_tfidf, X_train_backward_tfidf,X_train_forward_word2vec, X_train_backward_word2vec, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "model4 = CombinedBiGRU(X_train_forward_tfidf.shape[2],X_train_forward_word2vec.shape[2], 100,1, y_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model4.parameters(),lr = 0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs_forward_tfidf, inputs_backward_tfidf,inputs_forward_word2vec, inputs_backward_word2vec, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model4(inputs_forward_tfidf, inputs_backward_tfidf,inputs_forward_word2vec, inputs_backward_word2vec)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print('[Epoch %d/100] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/i))\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on training the model4 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4a1c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model4 is:  5.146279811859131\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model4.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_combined_gru = model4(X_test_forward_tfidf, X_test_backward_tfidf,\n",
    "                             X_test_forward_word2vec, X_test_backward_word2vec)\n",
    "y_pred_combined_gru = y_pred_combined_gru.detach().numpy()\n",
    "rslt = evaluateModel(y_pred_combined_gru,y_test,0.2)\n",
    "results.loc[len(results.index)]=['Combined training of BiGRUs',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model4 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058b15b",
   "metadata": {},
   "source": [
    "### Step 6: Show the results\n",
    "After we built the 5 models above, let's compare the performance of the 5 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7492cd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the 5 models are as the follows: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MiP</th>\n",
       "      <th>MiR</th>\n",
       "      <th>MiF1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiLSTM with TF-IDF</td>\n",
       "      <td>0.844880</td>\n",
       "      <td>0.418782</td>\n",
       "      <td>0.595707</td>\n",
       "      <td>0.491817</td>\n",
       "      <td>0.804495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiLSTM with word2vec</td>\n",
       "      <td>0.861234</td>\n",
       "      <td>0.460096</td>\n",
       "      <td>0.583818</td>\n",
       "      <td>0.514625</td>\n",
       "      <td>0.814443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiLSTM with TF-IDF and word2vec</td>\n",
       "      <td>0.859053</td>\n",
       "      <td>0.455739</td>\n",
       "      <td>0.610504</td>\n",
       "      <td>0.521890</td>\n",
       "      <td>0.829092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined training of BiLSTMs</td>\n",
       "      <td>0.869381</td>\n",
       "      <td>0.486158</td>\n",
       "      <td>0.643076</td>\n",
       "      <td>0.553714</td>\n",
       "      <td>0.842082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combined training of BiGRUs</td>\n",
       "      <td>0.876479</td>\n",
       "      <td>0.507595</td>\n",
       "      <td>0.658741</td>\n",
       "      <td>0.573374</td>\n",
       "      <td>0.856416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy       MiP       MiR      MiF1  \\\n",
       "0               BiLSTM with TF-IDF  0.844880  0.418782  0.595707  0.491817   \n",
       "1             BiLSTM with word2vec  0.861234  0.460096  0.583818  0.514625   \n",
       "2  BiLSTM with TF-IDF and word2vec  0.859053  0.455739  0.610504  0.521890   \n",
       "3     Combined training of BiLSTMs  0.869381  0.486158  0.643076  0.553714   \n",
       "4      Combined training of BiGRUs  0.876479  0.507595  0.658741  0.573374   \n",
       "\n",
       "        AUC  \n",
       "0  0.804495  \n",
       "1  0.814443  \n",
       "2  0.829092  \n",
       "3  0.842082  \n",
       "4  0.856416  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The performance of the 5 models are as the follows: \")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bc88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
