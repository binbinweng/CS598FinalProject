{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b92dc2",
   "metadata": {},
   "source": [
    "# Notebook for CS598 Final Project \n",
    "Author: Binbin Weng (binbinw2@illinois.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399241b",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The method to implement is from the paper, `A disease inference method based on symptom extraction and bidirectional Long Short Term Memory networks`, which introduces a method of multi-label classifier for disease inference with clinical text data. \n",
    "\n",
    "Since the data used in the paper is clinical text data, symptoms are first extracted from the clinical text data, and then a deep learning model, bidirectional Long Short Term Memory network (BiLSTM), with two different representations of the extracted symptoms, representations of TF-IDF (term frequency-inverse document frequency) and Word2Vec (an embedding method), is built to improve the performance of the classifier. \n",
    "\n",
    "Using this two representations of the extracted symptoms is because the method of TF-IDF can reflect the relation between symptom and target disease and the method of Word2Vec can reflect the relation between symptom and symptom, so that more information are involved in the modeling, which improves the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa01d93",
   "metadata": {},
   "source": [
    "## 2. Data\n",
    "The datasets used in the paper are two tables, `NOTEEVENTS` and `DIAGNOSES_ICD`, from MIMIC-III, which can be accessed through https://physionet.org/content/mimiciii/1.4/. \n",
    "\n",
    "After you access the datasets, please save it to the same folder as you save the notebook.\n",
    "\n",
    "Per the paper, the clinical texts used are the discharge summaries.\n",
    "\n",
    "The statistics about the two tables are as the follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d0eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wangguanshen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "537d344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## set seeds ##########################\n",
    "seed = 2023\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6febb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/85wt69jn4hlfh9nykgpdzzg80000gn/T/ipykernel_44909/2333017314.py:2: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  noteevents = pd.read_csv('NOTEEVENTS.csv.gz')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of NOTEEVENTS table is:  (2083180, 11)\n",
      "Categories of the clinical text:\n",
      "Nursing/other        822497\n",
      "Radiology            522279\n",
      "Nursing              223556\n",
      "ECG                  209051\n",
      "Physician            141624\n",
      "Discharge summary     59652\n",
      "Echo                  45794\n",
      "Respiratory           31739\n",
      "Nutrition              9418\n",
      "General                8301\n",
      "Rehab Services         5431\n",
      "Social Work            2670\n",
      "Case Management         967\n",
      "Pharmacy                103\n",
      "Consult                  98\n",
      "Name: CATEGORY, dtype: int64\n",
      "The shape of Discharge Summary table is:  (59652, 11)\n",
      "Number of unique patients in the discharge summary table:  41127\n",
      "Number of unique visits in the discharge summary table:  52726\n"
     ]
    }
   ],
   "source": [
    "########## processing NOTEVENTS Table #######################################\n",
    "noteevents = pd.read_csv('NOTEEVENTS.csv.gz')\n",
    "print(\"The shape of NOTEEVENTS table is: \", noteevents.shape)\n",
    "print(\"Categories of the clinical text:\")\n",
    "print(noteevents['CATEGORY'].value_counts())\n",
    "# select only data with the category of Discharge summary\n",
    "discharge_summary_data = noteevents[noteevents['CATEGORY']=='Discharge summary']\n",
    "print(\"The shape of Discharge Summary table is: \", discharge_summary_data.shape)\n",
    "# subject_id is patient id, hadm_id is visit id\n",
    "print(\"Number of unique patients in the discharge summary table: \",discharge_summary_data['SUBJECT_ID'].nunique())\n",
    "print(\"Number of unique visits in the discharge summary table: \",discharge_summary_data['HADM_ID'].nunique())\n",
    "#drop unnecessary columns from discharge summary data\n",
    "discharge_summary_text = discharge_summary_data.drop(['ROW_ID','CHARTDATE','CHARTTIME','STORETIME','CATEGORY','DESCRIPTION','CGID','ISERROR'],axis =1)\n",
    "#change datatype for following processing\n",
    "discharge_summary_text['SUBJECT_ID'] = discharge_summary_text['SUBJECT_ID'].astype(str)\n",
    "# function to concatenate contents in each column with the same 'HADM_ID'\n",
    "def concat_values(group):\n",
    "    return pd.Series({\n",
    "        'SUBJECT_concat': ' '.join(group['SUBJECT_ID']),\n",
    "        'TEXT_concat': ' '.join(group['TEXT'])\n",
    "    })\n",
    "discharge_summary_text = discharge_summary_text.groupby('HADM_ID').apply(concat_values)\n",
    "discharge_summary_text = discharge_summary_text.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac3cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of DIAGNOSES_ICD table is:  (651047, 5)\n",
      "Number of unique patients in the DIAGNOSES_ICD table:  46520\n",
      "Number of unique visits in the DIAGNOSES_ICD table:  58976\n"
     ]
    }
   ],
   "source": [
    "########## processing DIAGNOSES_ICD Table #######################################\n",
    "dig = pd.read_csv('DIAGNOSES_ICD.csv.gz')\n",
    "print(\"The shape of DIAGNOSES_ICD table is: \",dig.shape)\n",
    "print(\"Number of unique patients in the DIAGNOSES_ICD table: \", dig['SUBJECT_ID'].nunique())\n",
    "print(\"Number of unique visits in the DIAGNOSES_ICD table: \", dig['HADM_ID'].nunique())\n",
    "# drop unnecessary columns from diagnoses data\n",
    "diagnoses_data = dig.drop(['ROW_ID','SEQ_NUM'],axis = 1)\n",
    "#change datatype for following processing\n",
    "diagnoses_data['SUBJECT_ID'] = diagnoses_data['SUBJECT_ID'].astype(str)\n",
    "diagnoses_data['ICD9_CODE'] = diagnoses_data['ICD9_CODE'].astype(str)\n",
    "# function to concatenate contents in each column with the same 'HADM_ID'\n",
    "def concat_values2(group):\n",
    "    return pd.Series({\n",
    "        'SUBJECT_concat': ' '.join(group['SUBJECT_ID']),\n",
    "        'ICD_concat_CODEs': ','.join(group['ICD9_CODE'])\n",
    "    })\n",
    "diagnoses_data = diagnoses_data.groupby('HADM_ID').apply(concat_values2)\n",
    "diagnoses_data = diagnoses_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30659925",
   "metadata": {},
   "source": [
    "## 3. Extract symptoms from discharge summaries.\n",
    "The purpose of the paper is to use the clinical text data to build a multi-label classifier for disease inference. The first thing needs to do is to extract symptoms from the discharge summaries. It uses the techniques introduced by MetaMap to extract symptoms from the clinical text data. \n",
    "\n",
    "MetaMap provides a database which you can download and set up the environment for it so that you can run the extraction from your local side. Please go to the website page, https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/run-locally/MainDownload.html, to download related database and follow the instructions to set up the environment.  \n",
    "\n",
    "However, this method is very slow. I tried it. It took about 25 CPU hours to extract symptoms from about 1000 records. The dataset contains about 50,000 records, so obviously this method is not efficient enough.\n",
    "\n",
    "Luckily, MetaMap also provides the service of Batch MetaMap, where you can upload your file and it will help extract symptoms from your file and return results to you. So the following part of code is to prepare the data which you can upload to Batch MetaMap. After running the following part of code, two files will be generated, `merged_data.csv` and `data_for_metamapBatch_full.txt`. The file, `merged_data.csv` is for later modeling use which contains labels for each records. The file, `data_for_metamapBatch_full.txt`, is to submit in Batch MetaMap to get the symptoms for each records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_data_for_MetaMap\n",
    "prepare_data_for_MetaMap.processing_text_for_MetaMap(discharge_summary_text,diagnoses_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08b514",
   "metadata": {},
   "source": [
    "## 4. Get symptoms from Batch MetaMap\n",
    "Register an account on National Library of Medicine, https://www.nlm.nih.gov/. It may take some time to review your registration.\n",
    "\n",
    "After your registration gets approved, go to https://ii.nlm.nih.gov/Batch/UTS_Required/MetaMap.html,Enter, enter FULL Email Address (which is used to contact you when the job is finished), upload the data_for_metamapBatch_full.txt from the previous step, in the Out/Display Options select Fielded MMI output (-N), in the Batch Specific Options select Single Line Delimited Input w/ ID, in the I would like to only use specific Semantic Types, enter `sosy,dsyn,neop,fngs,bact,virs,cgab,acab,lbtr,inpo,mobd,comd,anab`, then submit Batch MetaMap. It then will give you a link to track your job. It takes about 3 days to process the data. When the job finishes, you will receive an email containing link for the output files. Download the `text.out` file and `text.out.ERR` file to the same folder you store your other data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7bd2b",
   "metadata": {},
   "source": [
    "## 5. Symptoms from Batch MetaMap\n",
    "Running the following part of code will help process the symptoms gotten from Batch MetaMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_symptoms\n",
    "symptoms = pd.read_csv('text.out.txt', delimiter='|', header=None,\n",
    "                       usecols=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "symptoms_final = prepare_symptoms.symptoms(symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7eb523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/85wt69jn4hlfh9nykgpdzzg80000gn/T/ipykernel_44909/2954109733.py:2: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  symptoms = pd.read_csv('text.out.txt', delimiter='|', header=None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of symptoms table is:  (47887, 3)\n"
     ]
    }
   ],
   "source": [
    "import prepare_symptoms\n",
    "symptoms = pd.read_csv('text.out.txt', delimiter='|', header=None,\n",
    "                       usecols=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "symptoms_final = prepare_symptoms.symptoms(symptoms)\n",
    "print(\"The shape of symptoms table is: \", symptoms_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccedf9a",
   "metadata": {},
   "source": [
    "## 6: Prepare data for modeling\n",
    "After we get the results from symptoms, we need to merge the symptoms with the labels. Then we can start prepare the data for modeling.\n",
    "\n",
    "1. Per the paper, just keep the first 50 symotoms if the number of symotoms in an observation is over 50; delete the observations with only one symotom\n",
    "2. Per the paper, just keep the most common 50 diseases, because the observations with the most common 50 diseases take 97% of the dataset\n",
    "3. Prepare the TF-IDF matrix. The way to get the TF-IDF matrix is as the follows:\n",
    "\n",
    "Each symptom i will be represented as vector ${S_i= (W_{i,1},W_{i,2},...,W_{i,d})}$, where ${W_{i,j}}$ is the strength of the association between symptom i and disease j. More specifically, ${W_{i,j}=TF_{i,j}*log{\\frac{N}{D_i}}}$, where $TF_{i,j}$ is the number of symptom i in the clinical texts data correlated with disease j, $N$ is the number of all diseases, $D_i$ is the number of diseases associated with symptom i. So the matrix should be of the shape (number of symptoms, number of diseases) \n",
    "\n",
    "4. Build the word2vec model for symtoms.\n",
    "5. Prepare forward and backward for both TF-IDF representation and word2vec representation of the symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee8b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique visits after merging with symptoms is:  (47887, 9)\n"
     ]
    }
   ],
   "source": [
    "################## merged the symptoms with the ICD codes with each visit ##########################\n",
    "merged_data = pd.read_csv('merged_data.csv')\n",
    "merged_data = merged_data.reset_index()\n",
    "symptom_ICD = pd.merge(merged_data,symptoms_final,on = 'index',how = 'inner')\n",
    "symptom_ICD.to_csv('symptom_ICD.csv',index = False)\n",
    "print(\"The number of unique visits after merging with symptoms is: \", symptom_ICD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fa8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## select symptoms and diseases ###############################################\n",
    "import prepare_data_for_modeling\n",
    "symptom_ICD = prepare_data_for_modeling.select_symptoms(symptom_ICD)\n",
    "symptom_ICD = prepare_data_for_modeling.select_diseases(symptom_ICD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d546338",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### generate dummy variables for symptoms and ICD codes #######################\n",
    "s1 = symptom_ICD.shape[1]\n",
    "symptom_ICD_new = (symptom_ICD['symptom_code_list'].apply(lambda x:','.join(x))).str.get_dummies(',')\n",
    "symptom_ICD_new = symptom_ICD.join(symptom_ICD_new)\n",
    "e1 = symptom_ICD_new.shape[1]\n",
    "\n",
    "s2 = symptom_ICD_new.shape[1]\n",
    "symptom_ICD_new = symptom_ICD_new.join((symptom_ICD_new['common50_icds'].apply(lambda x:','.join(x))).str.get_dummies(','))\n",
    "e2 = symptom_ICD_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a45692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (46659, 18146)\n",
      "Y shape:  (46659, 50)\n",
      "shape of W matrix:  (18146, 50)\n",
      "time spent on calculating TF-IDF matrix:  5243.654246807098\n"
     ]
    }
   ],
   "source": [
    "################## calculate TF-IDF Matrix #########################################\n",
    "start_time = time.time()\n",
    "X = np.array(symptom_ICD_new.iloc[:,s1:e1])\n",
    "Y = np.array(symptom_ICD_new.iloc[:,s2:e2])\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", Y.shape)\n",
    "item_tf =  np.zeros((X.shape[1],Y.shape[1]))\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(Y.shape[1]):\n",
    "        item_tf[i,j] = sum(X[:,i]*Y[:,j])\n",
    "D = []\n",
    "for i in range(X.shape[1]):\n",
    "    ans = 0\n",
    "    for j in range(Y.shape[1]):\n",
    "        if sum(X[:,i]*Y[:,j])>0:\n",
    "            ans+=1\n",
    "    D.append(ans)\n",
    "W = np.zeros((X.shape[1],Y.shape[1]))\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(Y.shape[1]):\n",
    "        W[i,j]=item_tf[i,j]*math.log(Y.shape[1]/D[i])\n",
    "print(\"shape of W matrix: \",W.shape)\n",
    "np.savetxt(\"tfidf_matrix.csv\",W,delimiter = ',')\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on calculating TF-IDF matrix: \", diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b406573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on word2vec:  18.809247970581055\n"
     ]
    }
   ],
   "source": [
    "################## build word2vec model #########################################\n",
    "start_time = time.time()\n",
    "sentences = symptom_ICD_new['symptom_code_list'].apply(lambda x: [y.replace(\" \",\"\") for y in x])\n",
    "embedding_model = Word2Vec(sentences, sg=1, window=5, vector_size=128, min_count=1, workers=4)\n",
    "embedding_model.save('word2vec_model')\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on word2vec: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db4ca82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## prepare X ###################################################\n",
    "numOfICD = 50\n",
    "# the representation of TF-IDF\n",
    "col = list(symptom_ICD_new.columns)\n",
    "col = col[s1:e1]\n",
    "col_map = {}\n",
    "for i in range(len(col)):\n",
    "    col_map[col[i].replace(\" \",\"\")] = i\n",
    "symptom_ICD_new['tfidf_x'] = list(np.zeros((symptom_ICD_new.shape[0],50,numOfICD)))\n",
    "symptom_ICD_new['tfidf_x_rev'] = list(np.zeros((symptom_ICD_new.shape[0],50,numOfICD)))\n",
    "for i in range(symptom_ICD_new.shape[0]): \n",
    "    l = [x.replace(\" \",\"\") for x in symptom_ICD_new['selected_symtom_code'][i]]\n",
    "    for j in range(len(l)):\n",
    "        symptom_ICD_new['tfidf_x'][i][j] = W[col_map[l[j]],:]\n",
    "        symptom_ICD_new['tfidf_x_rev'][i][len(l)-j-1]= W[col_map[l[j]],:]\n",
    "        \n",
    "# the representation of word2vec\n",
    "symptom_ICD_new['word2vec_x'] = list(np.zeros((symptom_ICD_new.shape[0],50,128)))\n",
    "symptom_ICD_new['word2vec_x_rev'] = list(np.zeros((symptom_ICD_new.shape[0],50,128)))\n",
    "for i in range(symptom_ICD_new.shape[0]): \n",
    "    l = [x.replace(\" \",\"\") for x in symptom_ICD_new['selected_symtom_code'][i]]\n",
    "    for j in range(len(l)):\n",
    "        symptom_ICD_new['word2vec_x'][i][j] = embedding_model.wv[l[j]]\n",
    "        symptom_ICD_new['word2vec_x_rev'][i][len(l)-j-1]= embedding_model.wv[l[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1390ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## prepare Y ###################################################\n",
    "symptom_ICD_new['y'] = symptom_ICD_new.apply(lambda row: row.iloc[s2:e2].to_list(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9067a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### dataframe for X and Y #######################################\n",
    "final_df = symptom_ICD_new.loc[:,['tfidf_x','tfidf_x_rev','word2vec_x','word2vec_x_rev','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75d109e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/85wt69jn4hlfh9nykgpdzzg80000gn/T/ipykernel_44909/4012697825.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1678402353079/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  X_forward_tfidf = torch.tensor(final_df[\"tfidf_x\"]).float()\n"
     ]
    }
   ],
   "source": [
    "######################## split data into training (80%) and testing data (20%) #################\n",
    "X_forward_tfidf = torch.tensor(final_df[\"tfidf_x\"]).float()\n",
    "X_backward_tfidf = torch.tensor(final_df[\"tfidf_x_rev\"]).float()\n",
    "X_forward_word2vec = torch.tensor(final_df[\"word2vec_x\"]).float()\n",
    "X_backward_word2vec = torch.tensor(final_df[\"word2vec_x_rev\"]).float()\n",
    "Y = torch.tensor(final_df[\"y\"].tolist()).float()\n",
    "X_train_forward_tfidf, X_test_forward_tfidf, X_train_backward_tfidf, X_test_backward_tfidf,X_train_forward_word2vec, X_test_forward_word2vec, X_train_backward_word2vec, X_test_backward_word2vec, y_train, y_test = train_test_split(X_forward_tfidf, X_backward_tfidf, X_forward_word2vec, X_backward_word2vec, Y, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024be30e",
   "metadata": {},
   "source": [
    "## 7: Modeling and evaluating\n",
    "After prepare the data for modeling, in the following part, we will build and compare several models with the prepared data. \n",
    "\n",
    "The models are:\n",
    "1. the bidirectional LSTM with the X representation of TF-IDF\n",
    "2. the bidirectional LSTM with the X representation of word2vec\n",
    "3. simply combine the results from the two models above\n",
    "4. training two bidirectional LSTMs with the X representations of both TF-IDF and word2vec together\n",
    "5. training two bidirectional GRUs with the X representations of both TF-IDF and word2vec together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae4a932",
   "metadata": {},
   "source": [
    "The reason that bidirection LSTM is chosen is becuase the data used here is text data, which has the charactistics of seriality, and LSTM is a good deep learning model to catch this charactistics. In addition, bidirectional LSTM can catch the sequential information from both the forward side and backward side.\n",
    "\n",
    "The structure of the bidirectional LSTM is as the follows:\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Layers</th>\n",
    "<th>Configuration</th>\n",
    "<th>Activation Function</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Forward_LSTM</td>\n",
    "<td>input size 50, hidden size 100</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Backward_LSTM</td>\n",
    "<td>input size 50, hidden size 100</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Dropout</td>\n",
    "<td>probability 0.8</td>\n",
    "<td>-</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Full connected</td>\n",
    "<td>input size 200, output size 50</td>\n",
    "<td>Sigmoid</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e93ac",
   "metadata": {},
   "source": [
    "The matrics to evaluate the models are four micro-averaging measurements: Precison (MiP), Recall (MiR), F1-score (MiF1) and AUC (area under the receiver operating characteristic curve). The measurements are defined as the follows:\n",
    "\n",
    "$MiP = \\frac{\\sum_{i,j}{y^j_i}{\\hat{y}^j_i}}{\\sum_{i,j}{\\hat{y}^j_i}}$\n",
    "\n",
    "$MiR = \\frac{\\sum_{i,j}{y^j_i}{\\hat{y}^j_i}}{\\sum_{i,j}{y^j_i}}$\n",
    "\n",
    "$MiF1=\\frac{2*MiP*MiR}{MiP+MiR}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7d83c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, forward_input, backward_input):\n",
    "        h0_forward = torch.zeros(self.num_layers, forward_input.size(0), self.hidden_size)\n",
    "        c0_forward = torch.zeros(self.num_layers, forward_input.size(0), self.hidden_size)\n",
    "        h0_backward = torch.zeros(self.num_layers, backward_input.size(0), self.hidden_size)\n",
    "        c0_backward = torch.zeros(self.num_layers, backward_input.size(0), self.hidden_size)\n",
    "        out_forward, _ = self.lstm(forward_input, (h0_forward, c0_forward))\n",
    "        out_backward, _ = self.lstm(backward_input, (h0_backward, c0_backward))\n",
    "        output = torch.cat((out_forward[:,-1,:], out_backward[:,0,:]), dim=1)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba674d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(pred,truth,cutoff):\n",
    "    label = np.array(((pred>cutoff)*1.0))\n",
    "    truth2 = np.array(truth)\n",
    "    acc = np.sum(label==truth2)/np.prod(truth2.shape)\n",
    "    mip = np.sum(label*truth2)/np.sum(label)\n",
    "    mir = np.sum(label*truth2)/np.sum(truth2)\n",
    "    mif1 = (2*mip*mir)/(mip+mir)\n",
    "    auc = roc_auc_score(truth2,np.array(pred))\n",
    "    return [acc,mip,mir,mif1,auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6804b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Model', 'Accuracy', 'MiP', 'MiR', 'MiF1', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df6371e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/100] loss: 0.321\n",
      "[Epoch 20/100] loss: 0.315\n",
      "[Epoch 30/100] loss: 0.309\n",
      "[Epoch 40/100] loss: 0.303\n",
      "[Epoch 50/100] loss: 0.300\n",
      "[Epoch 60/100] loss: 0.297\n",
      "[Epoch 70/100] loss: 0.294\n",
      "[Epoch 80/100] loss: 0.292\n",
      "[Epoch 90/100] loss: 0.291\n",
      "[Epoch 100/100] loss: 0.289\n",
      "time spent on training the model1 is:  2451.0875160694122\n"
     ]
    }
   ],
   "source": [
    "##################### build model1: the bidirectional LSTM with the X representation of TF-IDF ################\n",
    "start_time = time.time()\n",
    "train_data = torch.utils.data.TensorDataset(X_train_forward_tfidf, X_train_backward_tfidf, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "model = BiLSTM(X_train_forward_tfidf.shape[2], 100, 1,y_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs_forward, inputs_backward, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs_forward, inputs_backward)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print('[Epoch %d/100] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/i))\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on training the model1 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5a74188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model1 is:  1.9696598052978516\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_tfidf = model(X_test_forward_tfidf, X_test_backward_tfidf)\n",
    "y_pred_tfidf = y_pred_tfidf.detach().numpy()\n",
    "rslt = evaluateModel(y_pred_tfidf,y_test,0.2)\n",
    "results.loc[len(results.index)]=['BiLSTM with TF-IDF',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model1 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fc6a161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  Accuracy       MiP       MiR      MiF1       AUC\n",
      "0  BiLSTM with TF-IDF   0.84488  0.418782  0.595707  0.491817  0.804495\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d98c5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/{num_epochs}] loss: 0.316\n",
      "[Epoch 20/{num_epochs}] loss: 0.308\n",
      "[Epoch 30/{num_epochs}] loss: 0.301\n",
      "[Epoch 40/{num_epochs}] loss: 0.297\n",
      "[Epoch 50/{num_epochs}] loss: 0.293\n",
      "[Epoch 60/{num_epochs}] loss: 0.290\n",
      "[Epoch 70/{num_epochs}] loss: 0.288\n",
      "[Epoch 80/{num_epochs}] loss: 0.285\n",
      "[Epoch 90/{num_epochs}] loss: 0.283\n",
      "[Epoch 100/{num_epochs}] loss: 0.281\n",
      "time spent on training model2 is:  3197.1380009651184\n"
     ]
    }
   ],
   "source": [
    "##################### build model2: the bidirectional LSTM with the X representation of word2vec ################\n",
    "start_time = time.time()\n",
    "train_data = torch.utils.data.TensorDataset(X_train_forward_word2vec, X_train_backward_word2vec, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "model2 = BiLSTM(X_train_forward_word2vec.shape[2], 100, 1,y_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(),lr = 0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs_forward, inputs_backward, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(inputs_forward, inputs_backward)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print('[Epoch %d/{num_epochs}] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/i))\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on training model2 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bda2d319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model is:  2.74979305267334\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_word2vec = model2(X_test_forward_word2vec, X_test_backward_word2vec)\n",
    "y_pred_word2vec = y_pred_word2vec.detach().numpy()\n",
    "rslt = evaluateModel(y_pred_word2vec,y_test,0.2)\n",
    "results.loc[len(results.index)]=['BiLSTM with word2vec',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ea6a645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model  Accuracy       MiP       MiR      MiF1       AUC\n",
      "0    BiLSTM with TF-IDF  0.844880  0.418782  0.595707  0.491817  0.804495\n",
      "1  BiLSTM with word2vec  0.861234  0.460096  0.583818  0.514625  0.814443\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b2b037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model3 is:  0.13937115669250488\n"
     ]
    }
   ],
   "source": [
    "################## build model3: simply combine the results from model1 and model2 ################################\n",
    "start_time = time.time()\n",
    "rslt = evaluateModel(y_pred_tfidf*0.5+y_pred_word2vec*0.5,y_test,0.2)\n",
    "results.loc[len(results.index)]=['BiLSTM with TF-IDF and word2vec',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model3 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eea7bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model  Accuracy       MiP       MiR      MiF1  \\\n",
      "0               BiLSTM with TF-IDF  0.844880  0.418782  0.595707  0.491817   \n",
      "1             BiLSTM with word2vec  0.861234  0.460096  0.583818  0.514625   \n",
      "2  BiLSTM with TF-IDF and word2vec  0.859053  0.455739  0.610504  0.521890   \n",
      "\n",
      "        AUC  \n",
      "0  0.804495  \n",
      "1  0.814443  \n",
      "2  0.829092  \n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79cd6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build model4: training two bidirectional LSTMs with the X representations of both TF-IDF and word2vec together ################################\n",
    "class CombinedBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, input_size2,hidden_size, num_layers,output_size):\n",
    "        super(CombinedBiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_tfidf = nn.LSTM(input_size, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.lstm_word2vec = nn.LSTM(input_size2, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, forward_input_tfidf, backward_input_tfidf,forward_input_word2vec, backward_input_word2vec):\n",
    "        h0_forward_tfidf = torch.zeros(self.num_layers, forward_input_tfidf.size(0), self.hidden_size)\n",
    "        c0_forward_tfidf = torch.zeros(self.num_layers, forward_input_tfidf.size(0), self.hidden_size)\n",
    "        h0_backward_tfidf = torch.zeros(self.num_layers, backward_input_tfidf.size(0), self.hidden_size)\n",
    "        c0_backward_tfidf = torch.zeros(self.num_layers, backward_input_tfidf.size(0), self.hidden_size)\n",
    "        out_forward_tfidf, _ = self.lstm_tfidf(forward_input_tfidf, (h0_forward_tfidf, c0_forward_tfidf))\n",
    "        out_backward_tfidf, _ = self.lstm_tfidf(backward_input_tfidf, (h0_backward_tfidf, c0_backward_tfidf))\n",
    "        output_tfidf = torch.cat((out_forward_tfidf[:,-1,:], out_backward_tfidf[:,0,:]), dim=1)\n",
    "        output_tfidf = self.dropout(output_tfidf)\n",
    "        output_tfidf = self.fc(output_tfidf)\n",
    "        output_tfidf = self.sigmoid(output_tfidf)\n",
    "        \n",
    "        h0_forward_word2vec = torch.zeros(self.num_layers, forward_input_word2vec.size(0), self.hidden_size)\n",
    "        c0_forward_word2vec = torch.zeros(self.num_layers, forward_input_word2vec.size(0), self.hidden_size)\n",
    "        h0_backward_word2vec = torch.zeros(self.num_layers, backward_input_word2vec.size(0), self.hidden_size)\n",
    "        c0_backward_word2vec = torch.zeros(self.num_layers, backward_input_word2vec.size(0), self.hidden_size)\n",
    "        out_forward_word2vec, _ = self.lstm_word2vec(forward_input_word2vec, (h0_forward_word2vec, c0_forward_word2vec))\n",
    "        out_backward_word2vec, _ = self.lstm_word2vec(backward_input_word2vec, (h0_backward_word2vec, c0_backward_word2vec))\n",
    "        output_word2vec = torch.cat((out_forward_word2vec[:,-1,:], out_backward_word2vec[:,0,:]), dim=1)\n",
    "        output_word2vec = self.dropout(output_word2vec)\n",
    "        output_word2vec = self.fc(output_word2vec)\n",
    "        output_word2vec = self.sigmoid(output_word2vec)\n",
    "        \n",
    "        output = output_word2vec*0.5+output_tfidf*0.5\n",
    "                                    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d8618f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/100] loss: 0.310\n",
      "[Epoch 20/100] loss: 0.299\n",
      "[Epoch 30/100] loss: 0.292\n",
      "[Epoch 40/100] loss: 0.286\n",
      "[Epoch 50/100] loss: 0.281\n",
      "[Epoch 60/100] loss: 0.276\n",
      "[Epoch 70/100] loss: 0.273\n",
      "[Epoch 80/100] loss: 0.271\n",
      "[Epoch 90/100] loss: 0.268\n",
      "[Epoch 100/100] loss: 0.267\n",
      "time spent on training the model4 is:  5808.835932016373\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_data = torch.utils.data.TensorDataset(X_train_forward_tfidf, X_train_backward_tfidf,X_train_forward_word2vec, X_train_backward_word2vec, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "model3 = CombinedBiLSTM(X_train_forward_tfidf.shape[2],X_train_forward_word2vec.shape[2], 100,1, y_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(),lr = 0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs_forward_tfidf, inputs_backward_tfidf,inputs_forward_word2vec, inputs_backward_word2vec, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model3(inputs_forward_tfidf, inputs_backward_tfidf,inputs_forward_word2vec, inputs_backward_word2vec)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print('[Epoch %d/100] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/i))\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on training the model4 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9115c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model4 is:  4.616988182067871\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model3.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_combined = model3(X_test_forward_tfidf, X_test_backward_tfidf,\n",
    "                             X_test_forward_word2vec, X_test_backward_word2vec)\n",
    "y_pred_combined = y_pred_combined.detach().numpy()\n",
    "rslt = evaluateModel(y_pred_combined,y_test,0.2)\n",
    "results.loc[len(results.index)]=['Combined training of BiLSTMs',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model4 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abf8a7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Model  Accuracy       MiP       MiR      MiF1  \\\n",
      "0               BiLSTM with TF-IDF  0.844880  0.418782  0.595707  0.491817   \n",
      "1             BiLSTM with word2vec  0.861234  0.460096  0.583818  0.514625   \n",
      "2  BiLSTM with TF-IDF and word2vec  0.859053  0.455739  0.610504  0.521890   \n",
      "3     Combined training of BiLSTMs  0.869381  0.486158  0.643076  0.553714   \n",
      "\n",
      "        AUC  \n",
      "0  0.804495  \n",
      "1  0.814443  \n",
      "2  0.829092  \n",
      "3  0.842082  \n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed658c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build model5: training two bidirectional GRUs with the X representations of both TF-IDF and word2vec together ################################\n",
    "class CombinedBiGRU(nn.Module):\n",
    "    def __init__(self, input_size, input_size2,hidden_size, num_layers,output_size):\n",
    "        super(CombinedBiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru_tfidf = nn.GRU(input_size, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.gru_word2vec = nn.GRU(input_size2, hidden_size,num_layers,batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, forward_input_tfidf, backward_input_tfidf,forward_input_word2vec, backward_input_word2vec):\n",
    "        h0_forward_tfidf = torch.zeros(self.num_layers, forward_input_tfidf.size(0), self.hidden_size)\n",
    "        h0_backward_tfidf = torch.zeros(self.num_layers, backward_input_tfidf.size(0), self.hidden_size)\n",
    "        out_forward_tfidf, _ = self.gru_tfidf(forward_input_tfidf, h0_forward_tfidf)\n",
    "        out_backward_tfidf, _ = self.gru_tfidf(backward_input_tfidf, h0_backward_tfidf)\n",
    "        output_tfidf = torch.cat((out_forward_tfidf[:,-1,:], out_backward_tfidf[:,0,:]), dim=1)\n",
    "        output_tfidf = self.dropout(output_tfidf)\n",
    "        output_tfidf = self.fc(output_tfidf)\n",
    "        output_tfidf = self.sigmoid(output_tfidf)\n",
    "        \n",
    "        h0_forward_word2vec = torch.zeros(self.num_layers, forward_input_word2vec.size(0), self.hidden_size)\n",
    "        h0_backward_word2vec = torch.zeros(self.num_layers, backward_input_word2vec.size(0), self.hidden_size)\n",
    "        out_forward_word2vec, _ = self.gru_word2vec(forward_input_word2vec, h0_forward_word2vec)\n",
    "        out_backward_word2vec, _ = self.gru_word2vec(backward_input_word2vec, h0_backward_word2vec)\n",
    "        output_word2vec = torch.cat((out_forward_word2vec[:,-1,:], out_backward_word2vec[:,0,:]), dim=1)\n",
    "        output_word2vec = self.dropout(output_word2vec)\n",
    "        output_word2vec = self.fc(output_word2vec)\n",
    "        output_word2vec = self.sigmoid(output_word2vec)\n",
    "        \n",
    "        output = output_word2vec*0.5+output_tfidf*0.5                       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "710d1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/100] loss: 0.306\n",
      "[Epoch 20/100] loss: 0.293\n",
      "[Epoch 30/100] loss: 0.283\n",
      "[Epoch 40/100] loss: 0.277\n",
      "[Epoch 50/100] loss: 0.272\n",
      "[Epoch 60/100] loss: 0.269\n",
      "[Epoch 70/100] loss: 0.266\n",
      "[Epoch 80/100] loss: 0.264\n",
      "[Epoch 90/100] loss: 0.263\n",
      "[Epoch 100/100] loss: 0.261\n",
      "time spent on training the model4 is:  5426.055936098099\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_data = torch.utils.data.TensorDataset(X_train_forward_tfidf, X_train_backward_tfidf,X_train_forward_word2vec, X_train_backward_word2vec, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "model4 = CombinedBiGRU(X_train_forward_tfidf.shape[2],X_train_forward_word2vec.shape[2], 100,1, y_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model4.parameters(),lr = 0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs_forward_tfidf, inputs_backward_tfidf,inputs_forward_word2vec, inputs_backward_word2vec, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model4(inputs_forward_tfidf, inputs_backward_tfidf,inputs_forward_word2vec, inputs_backward_word2vec)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch+1)%10 ==0:\n",
    "        print('[Epoch %d/100] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/i))\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on training the model4 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4a1c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent on evaluating the model4 is:  5.146279811859131\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model4.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_combined_gru = model4(X_test_forward_tfidf, X_test_backward_tfidf,\n",
    "                             X_test_forward_word2vec, X_test_backward_word2vec)\n",
    "y_pred_combined_gru = y_pred_combined_gru.detach().numpy()\n",
    "rslt = evaluateModel(y_pred_combined_gru,y_test,0.2)\n",
    "results.loc[len(results.index)]=['Combined training of BiGRUs',rslt[0],rslt[1],rslt[2],rslt[3],rslt[4]]\n",
    "end_time = time.time()\n",
    "diff_time = end_time-start_time\n",
    "print(\"time spent on evaluating the model4 is: \",diff_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058b15b",
   "metadata": {},
   "source": [
    "## 8: Compare the results\n",
    "After we built the 5 models above, let's compare the performance of the 5 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7492cd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the 5 models are as the follows: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MiP</th>\n",
       "      <th>MiR</th>\n",
       "      <th>MiF1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiLSTM with TF-IDF</td>\n",
       "      <td>0.844880</td>\n",
       "      <td>0.418782</td>\n",
       "      <td>0.595707</td>\n",
       "      <td>0.491817</td>\n",
       "      <td>0.804495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiLSTM with word2vec</td>\n",
       "      <td>0.861234</td>\n",
       "      <td>0.460096</td>\n",
       "      <td>0.583818</td>\n",
       "      <td>0.514625</td>\n",
       "      <td>0.814443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiLSTM with TF-IDF and word2vec</td>\n",
       "      <td>0.859053</td>\n",
       "      <td>0.455739</td>\n",
       "      <td>0.610504</td>\n",
       "      <td>0.521890</td>\n",
       "      <td>0.829092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Combined training of BiLSTMs</td>\n",
       "      <td>0.869381</td>\n",
       "      <td>0.486158</td>\n",
       "      <td>0.643076</td>\n",
       "      <td>0.553714</td>\n",
       "      <td>0.842082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combined training of BiGRUs</td>\n",
       "      <td>0.876479</td>\n",
       "      <td>0.507595</td>\n",
       "      <td>0.658741</td>\n",
       "      <td>0.573374</td>\n",
       "      <td>0.856416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy       MiP       MiR      MiF1  \\\n",
       "0               BiLSTM with TF-IDF  0.844880  0.418782  0.595707  0.491817   \n",
       "1             BiLSTM with word2vec  0.861234  0.460096  0.583818  0.514625   \n",
       "2  BiLSTM with TF-IDF and word2vec  0.859053  0.455739  0.610504  0.521890   \n",
       "3     Combined training of BiLSTMs  0.869381  0.486158  0.643076  0.553714   \n",
       "4      Combined training of BiGRUs  0.876479  0.507595  0.658741  0.573374   \n",
       "\n",
       "        AUC  \n",
       "0  0.804495  \n",
       "1  0.814443  \n",
       "2  0.829092  \n",
       "3  0.842082  \n",
       "4  0.856416  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The performance of the 5 models are as the follows: \")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a71e10",
   "metadata": {},
   "source": [
    "#### Result 1 \n",
    "Based on the table, MiP, MiR, MiF1, and AUC of the BiLSTMs with the two representations are all higher than that of BiLSTM with only the representation of TF-IDF, which fully supports the claim in the paper that using the two representations of the symptoms in the modeling do improve the performance of the classifier.\n",
    "\n",
    "#### Result 2 \n",
    "Based on the table, MiR, MiF1 and AUC of the BiLSTMs with the two representations are higher than that of BiLSTM with only the representation of Word2Vec, which does not fully support the claim in the paper that using the two representations of the symptoms in the modeling do improve the performance of the classifier.\n",
    "\n",
    "\n",
    "#### Result 3 \n",
    "In the original paper, the two BiLSTMs are trained separately first and then the outputs of the two BiLSTMs are added together with weight to get the final results. I want to test if training the two BiLSTMs together would have better performance. Based on the results in the table, training the BiLSTMs together has higher MiP, MiR, MiF1, and AUC than the model that simply sums up the results from the two separately trained BiLSTMs with weight. So training the two BiLSTMs together has better performance.\n",
    "\n",
    "#### Result 4\n",
    "In addition, suprisingly, training two BiGRUs together has better performance than training the two BiLSTMs together. Since BiGRU is simpler than BiLSTM, it was expected that BiLSTM should have better performance because BiLSTM has better ability to catch the sequential information in text data. However, the results in the table show that the model of BiGRUs outperforms the model of BiLSTMs with all the measurements. \n",
    "\n",
    "The reason might be that since the symptoms are finally used in modeling instead of the original full texts, the long term memory is not important in predicting the results, while the short term memory or more specifically the memory from last time step is more important in predicting the results. The reason still needs to be tested with further experiments.\n",
    "\n",
    "Based on the runtime of training the BiLSTMs and BiGRUs and performance of these two methods, the method with BiGRUs is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd328c",
   "metadata": {},
   "source": [
    "## 9. Reference:\n",
    "[1]Donglin Guo, Guihua Duan, Ying Yu, Yaohang Li, Fang-Xiang Wu, Min Li: A disease inference method based on symptom extraction and bidirectional Long Short Term Memory networks \n",
    "\n",
    "[2]Parikshit Sondhi, Jimeng Sun, Hanghang Tong, ChengXiang Zhai: SympGraph: A Framework for Mining Clinical Notes through Symptom Relation Graphs\n",
    "\n",
    "[3]https://pytorch.org/\n",
    "\n",
    "[4]https://numpy.org/\n",
    "\n",
    "[5]https://pandas.pydata.org/\n",
    "\n",
    "[6]https://stackoverflow.com/\n",
    "\n",
    "[7]https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/documentation/Installation.html\n",
    "\n",
    "[8]https://scikit-learn.org/\n",
    "\n",
    "[9]https://physionet.org/content/mimiciii/1.4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df8437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
